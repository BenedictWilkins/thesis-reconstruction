{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8a66e622",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision\n",
    "import torchvision.transforms as T\n",
    "import pytorch_lightning as pl\n",
    "import torchsummary\n",
    "import numpy as np\n",
    "\n",
    "from types import SimpleNamespace\n",
    "from omegaconf import OmegaConf\n",
    "from hydra.utils import instantiate, get_class\n",
    "\n",
    "import tml\n",
    "import wandb\n",
    "wandb.finish() # if not finished\n",
    "\n",
    "class Module(pl.LightningModule):\n",
    "    \n",
    "    def __init__(self, model, criterion, optimiser):\n",
    "        super().__init__()\n",
    "        self.model = model = instantiate(model)\n",
    "        self.criterion = instantiate(criterion)\n",
    "        self.optimiser = instantiate(optimiser, _args_=[self.model.parameters()])\n",
    "        \n",
    "    def configure_optimizers(self):\n",
    "        return self.optimiser\n",
    "    \n",
    "    def training_step(self, batch, _):\n",
    "        x, = batch\n",
    "        y = self.model(x)\n",
    "        loss = self.criterion(y, x)\n",
    "        self.log(\"train/loss\", loss.item())\n",
    "        return loss\n",
    "        \n",
    "    def validation_step(self, batch, batch_i):\n",
    "        x, = batch\n",
    "        y = self.model(x)\n",
    "        self.log(\"validation/loss\", self.criterion(y, x).item())\n",
    "        if batch_i == 0: # log images on the first batch\n",
    "            y = self._reconstruction(y)\n",
    "            self.logger.log_image(\"validation/reconstruction\", self._get_image(x[:16],y[:16]))\n",
    "    \n",
    "    def test_step(self, batch, batch_i):\n",
    "        x, = batch\n",
    "        y = self._reconstruction(self.model(x))\n",
    "        return x, y, F.mse_loss(y, x, reduction='none').view(x.shape[0],-1).sum(-1)\n",
    "\n",
    "    def test_epoch_end(self, outputs):\n",
    "        x, y, score = [torch.cat(z) for z in zip(*outputs)]\n",
    "        index = np.argsort(-score.cpu().numpy()) # largest scores first\n",
    "        # show top anomalies according reconsruction error\n",
    "        x_ranked, y_ranked = x[index], y[index]\n",
    "        self.logger.log_image(\"test/top_ground_truth\", self._get_image(x_ranked[:128], n=16))\n",
    "        self.logger.log_image(\"test/top_reconstruction\", self._get_image(y_ranked[:128], n=16))\n",
    "        # show raw scores\n",
    "        self.logger.experiment.log({\"test/score\" : self._get_line_plot(score[index], columns=['x', 'score'], title=\"Score\")})\n",
    "\n",
    "    def _reconstruction(self, y):\n",
    "        if y is not None and \"logit\" in str(self.criterion).lower():\n",
    "            y = torch.sigmoid(y)\n",
    "        return y\n",
    "    \n",
    "    def _get_line_plot(self, x, y=None, columns=['x','y'], title=\"Line Plot\"):\n",
    "        if y is None:\n",
    "            y, x = x, torch.arange(x.shape[0])\n",
    "        x, y = x.cpu().numpy(), y.cpu().numpy()\n",
    "        data = [[i,j] for (i,j) in zip(x,y)]\n",
    "        table = wandb.Table(data=data, columns=columns)\n",
    "        return wandb.plot.line(table, columns[0], columns[1], title=title)\n",
    "    \n",
    "    def _get_image(self, *x, n=16):\n",
    "        x = torch.cat(x, dim=2)\n",
    "        x = torch.clip(x, 0, 1)\n",
    "        return [torchvision.utils.make_grid(img, nrow=n, pad_value=1) for img in torch.split(x, n)]\n",
    "\n",
    "class DataModule(pl.LightningDataModule):\n",
    "    \n",
    "    def __init__(self, batch_size):\n",
    "        super().__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.train_dataset = None\n",
    "        self.test_dataset = None\n",
    "        self.validate_dataset = None\n",
    "        self.train_val_split = 0.8\n",
    "    \n",
    "    def prepare_train_data(self):\n",
    "        transform = lambda x: x.unsqueeze(1).to(\"cuda:0\").float() / 255.\n",
    "        dataset = torchvision.datasets.MNIST(pathlib.Path(\"~/.data/MNIST/\").expanduser().resolve(), train=True, download=True)\n",
    "        data = dataset.data[:int(self.train_val_split*dataset.data.shape[0])]\n",
    "        self.train_dataset = TensorDataset(transform(data))\n",
    "        \n",
    "    def prepare_validation_data(self):\n",
    "        transform = lambda x: x.unsqueeze(1).to(\"cuda:0\").float() / 255.\n",
    "        dataset = torchvision.datasets.MNIST(pathlib.Path(\"~/.data/MNIST/\").expanduser().resolve(), train=True, download=True)\n",
    "        data = dataset.data[int(self.train_val_split*dataset.data.shape[0]):]\n",
    "        self.validate_dataset = TensorDataset(transform(data))\n",
    "    \n",
    "    def prepare_test_data(self):\n",
    "        transform = lambda x: x.unsqueeze(1).to(\"cuda:0\").float() / 255.\n",
    "        dataset = torchvision.datasets.MNIST(pathlib.Path(\"~/.data/MNIST/\").expanduser().resolve(), train=False, download=True)\n",
    "        self.test_dataset = TensorDataset(transform(dataset.data))\n",
    "        \n",
    "    def prepare_data(self):\n",
    "        self.prepare_train_data()\n",
    "        self.prepare_validation_data()\n",
    "        self.prepare_test_data()\n",
    "    \n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_dataset, batch_size=self.batch_size, shuffle=True)    \n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.validate_dataset, batch_size=self.batch_size, shuffle=False)\n",
    "    \n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.test_dataset, batch_size=self.batch_size, shuffle=False)\n",
    "    \n",
    "class MLPAutoEncoder(nn.Sequential):\n",
    "    \n",
    "    def __init__(self, input_shape, latent_shape, output_activation=nn.Identity()):\n",
    "        self.input_shape = tml.shape.as_shape(input_shape)\n",
    "        self.latent_shape = tml.shape.as_shape(latent_shape)\n",
    "        latent_size = np.prod(self.latent_shape)\n",
    "        input_size = np.prod(self.input_shape)\n",
    "        layers = [\n",
    "            tml.module.View(input_shape, (input_size,)),\n",
    "            nn.Linear(input_size, 512), nn.LeakyReLU(),\n",
    "            nn.Linear(512, latent_size), nn.LeakyReLU(), \n",
    "            nn.Linear(latent_size, 512), nn.LeakyReLU(),\n",
    "            nn.Linear(512, input_size), output_activation,\n",
    "            tml.module.View((input_size,), self.input_shape)\n",
    "        ]\n",
    "        super().__init__(*layers)\n",
    "            \n",
    "class ConvAutoEncoder(nn.Sequential):\n",
    "    \n",
    "    def __init__(self, input_shape, latent_shape, output_activation=nn.Identity()):\n",
    "        self.input_shape = tml.shape.as_shape(input_shape)\n",
    "        self.latent_shape = tml.shape.as_shape(latent_shape)\n",
    "        latent_size = np.prod(self.latent_shape)\n",
    "        input_size = np.prod(self.input_shape)\n",
    "        # ignore latent_shape ? \n",
    "        layers = [\n",
    "            nn.Conv2d(1, 8, kernel_size=7, stride=1), nn.LeakyReLU(),\n",
    "            nn.Conv2d(8, 16, kernel_size=7, stride=1), nn.LeakyReLU(),\n",
    "            nn.Conv2d(16, 32, kernel_size=5, stride=1), nn.LeakyReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=5, stride=1), nn.LeakyReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=5, stride=1), nn.LeakyReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=4, stride=1), nn.LeakyReLU(),\n",
    "            \n",
    "            nn.ConvTranspose2d(64, 64, kernel_size=4, stride=1), nn.LeakyReLU(),\n",
    "            nn.ConvTranspose2d(64, 64, kernel_size=5, stride=1), nn.LeakyReLU(),\n",
    "            nn.ConvTranspose2d(64, 32, kernel_size=5, stride=1), nn.LeakyReLU(),\n",
    "            nn.ConvTranspose2d(32, 16, kernel_size=5, stride=1), nn.LeakyReLU(),\n",
    "            nn.ConvTranspose2d(16, 8, kernel_size=7, stride=1), nn.LeakyReLU(),\n",
    "            nn.ConvTranspose2d(8, 1, kernel_size=7, stride=1), output_activation\n",
    "        ]\n",
    "        super().__init__(*layers)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dc59ce7",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type            | Params\n",
      "----------------------------------------------\n",
      "0 | model     | ConvAutoEncoder | 477 K \n",
      "1 | criterion | MSELoss         | 0     \n",
      "----------------------------------------------\n",
      "477 K     Trainable params\n",
      "0         Non-trainable params\n",
      "477 K     Total params\n",
      "1.911     Total estimated model params size (MB)\n",
      "/home/ben/anaconda3/envs/PhD/lib/python3.8/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:631: UserWarning: Checkpoint directory /home/ben/Documents/repos/thesis/thesis-reconstruction/None/version_None/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation sanity check: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ben/anaconda3/envs/PhD/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:132: UserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mbenedict-wilkins\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.11 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"http://127.0.0.1:8080/benedict-wilkins/thesis-reconstruction/runs/27lbneb5\" target=\"_blank\">dauntless-serenity-35</a></strong> to <a href=\"http://127.0.0.1:8080/benedict-wilkins/thesis-reconstruction\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ben/anaconda3/envs/PhD/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:132: UserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7dd0ad5956c41e4812b50837c17c9ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/ben/anaconda3/envs/PhD/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:132: UserWarning: The dataloader, test_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62960c75be9a4b1db1e0aca4171d038f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{}\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 16053... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value=' 39.30MB of 39.30MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#torchsummary.summary(MLPAutoEncoder(input_shape, latent_shape), device=\"cpu\", input_size=input_shape)\n",
    "#torchsummary.summary(ConvAutoEncoder(input_shape, latent_shape), device=\"cpu\", input_size=input_shape)\n",
    "\n",
    "config = OmegaConf.create(\n",
    "\"\"\" \n",
    "input_shape : [1,28,28]\n",
    "latent_shape : [16]\n",
    "batch_size : 512\n",
    "learning_rate : 0.0005\n",
    "\n",
    "module:\n",
    "    _target_ : __main__.Module\n",
    "    optimiser : \n",
    "        _target_ : torch.optim.Adam\n",
    "        lr : ${learning_rate}\n",
    "    model : \n",
    "        _target_ : __main__.ConvAutoEncoder\n",
    "        input_shape : ${input_shape}\n",
    "        latent_shape: ${latent_shape}\n",
    "        \n",
    "    criterion: \n",
    "        _target_ : torch.nn.MSELoss\n",
    "        \n",
    "data_module: \n",
    "    _target_ : __main__.DataModule\n",
    "    batch_size : ${batch_size}\n",
    "    \n",
    "trainer:\n",
    "    _target_: pytorch_lightning.Trainer\n",
    "    gpus: 1\n",
    "    max_epochs: 30\n",
    "    min_epochs: 10\n",
    "    check_val_every_n_epoch: 4\n",
    "    log_every_n_steps: 10\n",
    "    logger: \n",
    "        _target_: pytorch_lightning.loggers.WandbLogger\n",
    "        project: thesis-reconstruction\n",
    "        log_model: all\n",
    "        mode: online\n",
    "\"\"\")\n",
    "\n",
    "OmegaConf.resolve(config)\n",
    "module = instantiate(config.module, _recursive_=False)\n",
    "data_module = instantiate(config.data_module)\n",
    "\n",
    "trainer = instantiate(config.trainer)\n",
    "trainer.fit(module, datamodule=data_module)\n",
    "trainer.test(module, datamodule=data_module)\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8fedd06",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PhD",
   "language": "python",
   "name": "phd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
